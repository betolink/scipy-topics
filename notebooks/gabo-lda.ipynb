{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA and Gabriel García Márquez\n",
    " - A transgression to the literary world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #1: Import requirements and declare some helper functions, \n",
    "# yeah I know what you're thinking... this could be part of a library\n",
    "\n",
    "import urllib\n",
    "import json\n",
    "import string\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# We are using spacy as a parser so we disable their other capabilities to speed up things\n",
    "nlp = spacy.load('es', disable=['tagger', 'ner'])\n",
    "\n",
    "with open('../data/100_años_de_soledad.txt', 'r') as file:\n",
    "    corpus = file.read().replace('\\n', '').split('----')\n",
    "\n",
    "# Helper function\n",
    "def flatten(top_list):\n",
    "    for inner in top_list:\n",
    "        if isinstance(inner, (list,tuple)):\n",
    "            for j in flatten(inner):\n",
    "                yield j\n",
    "        else:\n",
    "            yield inner\n",
    "\n",
    "# Function to clean up the documents, lematizes words to their regular form.\n",
    "def clean_sentences(doc):\n",
    "    doc = nlp(doc)\n",
    "    processed_sentences = []\n",
    "    for num, sentence in enumerate(doc.sents):\n",
    "        tokens = [token.lemma_.strip().lower() for token in sentence if token.lemma_ not in string.punctuation]\n",
    "        cleaned_sentence = [token for token in tokens if token != '-pron-']\n",
    "        final_sentence = [token for token in cleaned_sentence if token not in nlp.Defaults.stop_words and len(token)>1]\n",
    "        processed_sentences.append(final_sentence)\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #2 parsing the corpus to generate sentences\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "gensim_unigram_documents = []\n",
    "document_list = []\n",
    "# unigram sentences is going to be used to train our bigram phraser\n",
    "unigram_sentences = []\n",
    "\n",
    "for page in corpus:\n",
    "    text = clean_sentences(page)\n",
    "    document_list.append({'text': text, 'bigrams': ''})\n",
    "    for sentence in text:\n",
    "        unigram_sentences.append(sentence)\n",
    "        \n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_phraser = Phraser(bigram_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #3 creating our bigrams\n",
    "\n",
    "# bigram corpus will contain an array of documents and their tokens, with bigram tokens included\n",
    "bigram_corpus = []\n",
    "\n",
    "for doc in document_list:\n",
    "    bigram_sentences = []\n",
    "    for unigram_sentence in doc['text']:\n",
    "        bigram_sentence = ' '.join(bigram_phraser[unigram_sentence])\n",
    "        bigram_sentences.append(bigram_sentence)\n",
    "    bigram_tokens = list(flatten(bigram_sentences))\n",
    "    bigrams = ' '.join(bigram_tokens)\n",
    "    doc['bigrams'] = bigram_sentences\n",
    "    bigram_corpus.append(bigrams.split())\n",
    "\n",
    "# Let's print a page from our corpus, note the difference between the not lemmatized/bigramized sentences and the ones that are\n",
    "print(document_list[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LDA model using Gensim a library for topic modeling, the output is a list of topics present in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Using GENSIM to do topic modelling, this cell takes some time... hang on.\n",
    "\n",
    "# num pases should be adjusted, 10 is just a guesstimate of when convergence will be achieved.\n",
    "num_passes = 10\n",
    "num_topics = 13\n",
    "words_per_topic = 7\n",
    "print_topics = False\n",
    "filename = 'topics' + str(num_topics) + '.html'\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(bigram_corpus)\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in bigram_corpus]\n",
    "\n",
    "lda_model = LdaMulticore(lda_corpus,\n",
    "                         num_topics=num_topics,\n",
    "                         id2word=dictionary,\n",
    "                         passes=num_passes,\n",
    "                         workers=8\n",
    "                        )\n",
    "\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "if print_topics:\n",
    "    print (\"Topic List: \\n\")\n",
    "    for topic in topics:\n",
    "        t = str((int(topic[0])+ 1))\n",
    "        print('Topic ' + t + ': ', topic[1:])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "ldaviz = pyLDAvis.gensim.prepare(corpus=lda_corpus,\n",
    "                        topic_model=lda_model,\n",
    "                        dictionary=dictionary,\n",
    "                        sort_topics=False)\n",
    "\n",
    "print (\"\\nPyLDAVis: \\n\")\n",
    "print('link to file: ')\n",
    "display(HTML('<a href=\"{}\" target=\"_blank\">PyLDAviz</a> '.format(filename)))\n",
    "pyLDAvis.save_html(ldaviz, filename)\n",
    "pyLDAvis.display(ldaviz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a trained model we can classify a new unseen document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Classifying an unseen document using our GENSIM model\n",
    "\n",
    "# this is from another García Márquez book, Love in the Time of Cholera\n",
    "\n",
    "unseen_document = \"\"\"\n",
    "Lo más absurdo de la situación de ambos era que nunca parecieron tan felices en público \n",
    "como en aquellos años de infortunio. Pues en realidad fueron los años de sus victorias mayores \n",
    "sobre la hostilidad soterrada de un medio que no se resignaba a admitirlos como eran: distintos y novedosos, \n",
    "y por tanto transgresores del orden tradicional.\n",
    " \"\"\"\n",
    "\n",
    "parsed_doc = list(flatten(clean_sentences(unseen_document)))\n",
    "vec = dictionary.doc2bow(parsed_doc)\n",
    "predicted_topics = lda_model[vec]\n",
    "predicted_topics = [(p[0]+1, p[1]) for p in predicted_topics]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try some Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Building LDA with scikit-learn\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "scikit_corpus = []\n",
    "for doc in document_list:\n",
    "    for sent in doc['bigrams']:\n",
    "        scikit_corpus.append(sent)\n",
    "    \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(scikit_corpus)\n",
    "\n",
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_topics=10,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Plotting our model using Singular Value Decomposition\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "clusters = KMeans(n_clusters=13, random_state=100).fit_predict(lda_output)\n",
    "\n",
    "# Build the Singular Value Decomposition(SVD) model\n",
    "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
    "lda_output_svd = svd_model.fit_transform(lda_output)\n",
    "\n",
    "# X and Y axes of the plot using SVD decomposition\n",
    "x = lda_output_svd[:, 0]\n",
    "y = lda_output_svd[:, 1]\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x, y, c=clusters)\n",
    "plt.ylabel('Component 2')\n",
    "plt.xlabel('Component 1')\n",
    "plt.title(\"Segregation of Topic Clusters of Gabriel García Marquéz's 100 Years of solitude\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
